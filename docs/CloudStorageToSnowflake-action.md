# Cloud Storage to Snowflake Action


Description
-----------
Loads data from external or internal staged files to an existing [Snowflake](https://www.snowflake.com/product/) table.

Plugin works on top of COPY INTO. Please see its 
[docs](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html)
for more information.

Configuration
-------------

### Basic

**Label:** Label for UI.

**Account Name:** Snowflake account name and region and possibly a cloud specifier. (Part of the URL you use to 
log in to Snowflake, minus the "snowflakecomputing.com"). E.g. "myaccount.us-central1.gcp".

**Database:** Database that contains the target table.

**Schema:** Schema that contains the target table.

**Warehouse:** Warehouse to use for the data load operation (i.e. compute resources).

**Role:** Role to use (e.g. `ACCOUNTADMIN`).

**Source Type:** The source of the data to load. Possible values: `From Path`, `From Query`.

**Source Path:** Internal or external location where the files containing data are staged.

Examples of internal locations formats:
```
@~[/{path}]
@[{namespace}.]{int_stage_name}[/{path}]
@[{namespace}.]%{table_name}[/{path}]
```

Examples of external locations formats:
```
@[{namespace}.]{ext_stage_name}[/{path}]
s3://{bucket}[/{path}]
gcs://{bucket}[/{path}]
azure://{account}.blob.core.windows.net/{container}[/{path}]
```

**Source Query:** An SQL query which specifies an explicit set of fields/columns (separated by commas) 
to load from the staged data files. The fields/columns are selected from the files using a standard SQL query.
 
Example:
```
SELECT name, address, phone from s3://bucket/path
```

**Destination Table:** Snowflake table to load data into. Needs to be present.

### Credentials

**Username:** Username to use to connect to your Snowflake account.

**Password:** Password to use to connect to your Snowflake account. Not necessary for key pair or OAuth2 authentication.

### Key Pair Authentication

**Key Pair Authentication Enabled:** If true, plugin will perform key pair authentication.

**Private Key:** Private key contents.

**Key File Passphrase:** Passphrase for the private key file.

### OAuth2

To use OAuth2, user must create a snowflake security integration for it.
For more info see [Introduction to OAuth in Snowflake](https://docs.snowflake.com/en/user-guide/oauth-intro.html)

**OAuth2 Enabled:** If true, plugin will perform OAuth2 authentication.

**Client ID:** Client ID obtained via system function 
[SYSTEM$SHOW_OAUTH_CLIENT_SECRETS](https://docs.snowflake.com/en/sql-reference/functions/system_show_oauth_client_secrets.html)

**Client Secret:** Client ID obtained via system function 
[SYSTEM$SHOW_OAUTH_CLIENT_SECRETS](https://docs.snowflake.com/en/sql-reference/functions/system_show_oauth_client_secrets.html)

**Refresh Token:** Token used to receive accessToken, which is end product of OAuth2. Must be generated by user.

### Cloud Provider Parameters
More information on this section can be found on 
[the official site](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html#additional-cloud-provider-parameters)

**Use Cloud Provider Parameters:** If true, plugin will use cloud provider parameters to authenticate to source path.
Required only for loading from an external private/protected cloud storage location; not required for public 
buckets/containers.

**Cloud Provider:** Cloud provider name. Possible values: `GCP`, `AWS`, `Microsoft Azure`.

**Storage Integration:** Specifies the name of the storage integration used to delegate authentication responsibility 
for external cloud storage to a Snowflake identity and access management (IAM) entity. For more details, see 
[CREATE STORAGE INTEGRATION](https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html).
                         
Note. We highly recommend the use of storage integrations. This avoids the need to supply cloud storage 
when creating stages or loading data.

---

#### Amazon S3 specific parameters:

The credentials you specify depend on whether you associated the Snowflake access permissions 
for the bucket with an AWS IAM (Identity & Access Management) user or role:

**Key Id:** Amazon key ID.

**Secret Key:** Amazon secret key.

**Token:** Amazon token.

---

#### Microsoft Azure specific parameters:

**SAS Token:** SAS (shared access signature) token for connecting to Azure and accessing the 
private/protected container where the files containing data are staged. Credentials are generated by Azure.

---

**Files Encrypted:** If true, plugin will perform loading from encrypted files.

**Encryption type:** Encryption of the files. Possible values:
- `AWS_CSE:` Client-side encryption (requires a `Master Key` value). Currently, the client-side master key 
you provide can only be a symmetric key. Note that, when a `Master Key` value is provided, Snowflake assumes 
TYPE = AWS_CSE (i.e. when a MASTER_KEY value is provided, TYPE is not required).
- `AWS_SSE_S3:` Server-side encryption that requires no additional encryption settings.
- `AWS_SSE_KMS:` Server-side encryption that accepts an optional `KMS Key Id` parameter.
- `AZURE_CSE:` Client-side encryption; requires a `Master Key` value.
- `GCS_SSE_KMS:` Server-side encryption that accepts an optional `KMS Key Id` parameter.

**Master Key:** Client-side master key that Snowflake uses to encrypt the files containing the 
unloaded data. The master key must be a 128-bit or 256-bit key in Base64-encoded form. Required by
`AWS_CSE` and `AZURE_CSE` encryption types.

**KMS Key Id:** ID for the KMS-managed key that is used to encrypt files unloaded into the bucket. 
If no value is provided, your default KMS key ID is used to encrypt files on unload. The value is optionally
and used by the following encryption types: `AWS_SSE_KMS`, `GCS_SSE_KMS`.

### File Format

**File Format Filtering Policy:** Policy used to filter source files. Possible value:
- `Undefined` Do not filter the files.
- `By File Type` Filter using `Format Type` configuration.
- `By Existing Format Specification` Filter by existing format saved in Snowflake.

**Format Name:** Existing named file format to use for loading data into the table. The named file 
format determines the format type (CSV, JSON, etc.), as well as any other format options, for the data files. 
For more information, see [CREATE FILE FORMAT.](https://docs.snowflake.com/en/sql-reference/sql/create-file-format.html)

**Format Type:** Type of files to load into the table. Supported values; CSV, JSON, AVRO, ORC, PARQUET, XML.

**Format Type Options:** Depending on the file format type specified (`Format Type` property) 
you can include one or more of the format-specific options. Find the options list 
[here.](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html#format-type-options-formattypeoptions).

### Advanced

**Files:** List of one or more files names (separated by commas) to be loaded. The files must already have 
been staged in either the Snowflake internal location or external location specified in the command.
The maximum number of files names that can be specified is 1000.
           
Note. For external stages only (Amazon S3, Google Cloud Storage, or Microsoft Azure), the file path is set by concatenating 
the URL in the stage definition and the list of resolved file names.
However, Snowflake doesnâ€™t insert a separator implicitly between the path and file names. 
You must explicitly include a separator (/) either at the end of the URL in the stage definition or 
at the beginning of each file name specified in this parameter.

**Pattern:** Regular expression pattern string, enclosed in single quotes, specifying the file names 
and/or paths to match. 

Tip. For the best performance, try to avoid applying patterns that filter on a large number of files.

**Copy Options:** List of copy options. Find them 
[here.](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html#copy-options-copyoptions)

**Connection Arguments:** List of arbitrary string tag/value pairs as connection arguments. See: [JDBC Driver Connection String.](https://docs.snowflake.com/en/user-guide/jdbc-configure.html#jdbc-driver-connection-string)
